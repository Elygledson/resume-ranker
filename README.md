# ResumeRanker ‚Äî An√°lise Inteligente de Curr√≠culos com OCR + LLM

Fabio, gerente de Talent Acquisition da startup TechMatch, enfrentava um grande desafio: analisar manualmente dezenas de curr√≠culos em PDF ou imagem todos os dias. O processo era repetitivo, demorado e o impedia de focar em decis√µes estrat√©gicas de contrata√ß√£o.

Pensando nisso, esta aplica√ß√£o foi desenvolvida para automatizar a leitura, sumariza√ß√£o e ranqueamento de curr√≠culos usando t√©cnicas de OCR e modelos de linguagem (LLMs). Fabio agora pode simplesmente enviar os arquivos, fazer perguntas do tipo _"Qual curr√≠culo melhor se encaixa nessa vaga?"_ e receber respostas precisas com justificativas ‚Äî economizando horas de trabalho.

---

## Funcionalidades

- Upload de m√∫ltiplos documentos (PDF, JPG/PNG)
- Extra√ß√£o de texto via OCR (EasyOCR)
- Gera√ß√£o de sum√°rios individuais por curr√≠culo
-  Resposta a queries com justificativas baseadas nos curr√≠culos
- Registro de logs de uso com:
  - `request_id`
  - `user_id`
  - `timestamp`
  - `query`
  - `result`
- Sem armazenamento completo dos arquivos (apenas metadados)
- Empacotado com Docker
- Swagger interativo com exemplos de uso

---

## Tecnologias Utilizadas

- **FastAPI** ‚Äî API RESTful com documenta√ß√£o autom√°tica (Swagger/OpenAPI)  
- **EasyOCR** ‚Äî Extra√ß√£o de texto de imagens e PDFs  
- **MongoDB** ‚Äî Banco de dados n√£o relacional para armazenamento de logs  
- **Celery + Redis** ‚Äî Processamento ass√≠ncrono e fila de tarefas  
- **Ollama ou Gemini** ‚Äî Servi√ßo de IA para gera√ß√£o de respostas e sum√°rios  
- **Docker** ‚Äî Empacotamento e execu√ß√£o da aplica√ß√£o  

---

## Fluxo da Aplica√ß√£o

1. Usu√°rio envia documentos (PDFs ou imagens) via API.
2. Os arquivos s√£o salvos no storage e os nomes s√£o enviados para um **worker Celery**.
3. O worker:
   - Extrai texto com OCR
   - Resume o conte√∫do
   - Gera embeddings
   - Calcula a similaridade com a vaga usando **dist√¢ncia do cosseno**
4. Apenas documentos com **score > 50%** s√£o enviados para avalia√ß√£o do modelo LLM.
5. O modelo retorna os curr√≠culos que mais combinam com a vaga, com **justificativas claras**.
6. A execu√ß√£o √© registrada no MongoDB para fins de auditoria e rastreabilidade.

As imagens abaixo ilustram o pipeline de execu√ß√£o da aplica√ß√£o e a arquitetura geral do sistema:
## Pipeline da Aplica√ß√£o
![Pipeline de Execu√ß√£o](pipeline.png)
## Diagrama da Arquitetura
![Diagrama da Arquitetura](arquitetura.png)

---

## Instala√ß√£o

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/Elygledson/resume-ranker
cd resume-ranker
```

---

## Configure o .env

### 2. Crie um arquivo .env com o seguinte conte√∫do:

> Voc√™ pode **alternar o servi√ßo de IA** (entre `ollama` e `gemini`) apenas alterando a vari√°vel `AI_SERVICE_NAME` ‚Äî gra√ßas √† implementa√ß√£o dos padr√µes **Factory** e **Strategy**, n√£o √© necess√°rio modificar o c√≥digo para essa troca.


```bash
# REDIS
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# AI SERVICE
AI_SERVICE_NAME=ollama  # ou gemini
AI_SERVICE_KEY=http://localhost:11434/api  # ou sua chave da Gemini

# MONGODB
MONGO_INITDB_ROOT_PORT=27017
MONGO_INITDB_ROOT_DBNAME=teste
MONGO_INITDB_ROOT_HOST=localhost
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=admin123
```

---

## Execute com Docker Compose

```bash
docker compose up -d --build
```

## Como usar a aplica√ß√£o?

Para utilizar a aplica√ß√£o, √© necess√°rio fazer requisi√ß√µes HTTP utilizando ferramentas como **Postman**, **Swagger UI** ou qualquer cliente de API de sua prefer√™ncia.

A documenta√ß√£o interativa da API est√° dispon√≠vel em:

üîó **http://localhost:8000/doc**

Nela, voc√™ encontrar√° todos os endpoints organizados por m√≥dulos:

---

### üîπ M√≥dulos dispon√≠veis

- **Analyzer**
- **Logs**

---

### üîç 1. Analyzer

Este m√≥dulo √© respons√°vel por iniciar a an√°lise dos curr√≠culos.

Voc√™ deve fazer uma requisi√ß√£o `POST` para o endpoint `/analyze-resume`, enviando os seguintes campos via **form-data**:

- `files`: Um ou mais arquivos (PDF, JPEG, PNG)
- `request_id`: Identificador √∫nico da requisi√ß√£o (UUID)
- `user_id`: Identificador do usu√°rio solicitante (UUID)
- `query`: *(opcional)* Texto da vaga ou pergunta a ser usada na an√°lise

#### Resposta esperada:
A API ir√° retornar um `log_id`.  
Esse ID identifica a requisi√ß√£o de an√°lise e pode ser usado para consultar o resultado posteriormente.

O log inicialmente ter√° o status `PROCESSING`, pois os arquivos est√£o sendo analisados de forma ass√≠ncrona por um worker.

**Destaque Importante:**  
Gra√ßas √† implementa√ß√£o de padr√µes de projeto como **Factory** e **Strategy**, √© poss√≠vel **adicionar ou trocar servi√ßos de an√°lise, OCR ou LLM de forma r√°pida e modular**, sem alterar o n√∫cleo da aplica√ß√£o. Isso garante escalabilidade e facilidade de manuten√ß√£o.

---

### 2. Verificar status da an√°lise

Com o `log_id` em m√£os, voc√™ pode consultar o resultado da an√°lise utilizando o endpoint:

GET /logs/{log_id}

#### Poss√≠veis status:
- `PROCESSING`: A an√°lise ainda est√° em andamento.
- `PROCESSED`: A an√°lise foi conclu√≠da com sucesso.
- `FAILED`: Ocorreu uma falha no processamento.

Se o status for `PROCESSED`, o log conter√° tamb√©m o resultado da an√°lise ‚Äî incluindo os curr√≠culos mais compat√≠veis com a vaga, acompanhados de justificativas claras.

### 3. Oportunidades de melhoria:
- Realizar testes mais abrangentes para garantir robustez e confiabilidade.

- Adicionar um servi√ßo de armazenamento (por exemplo, MinIO) para gerenciar arquivos de forma mais eficiente.

- Criar uma valida√ß√£o pr√©-processamento que verifique se o arquivo enviado √© realmente um curr√≠culo antes de prosseguir com o processamento.

### 4. Observa√ß√µes:

Se voc√™ n√£o pretende utilizar o Ollama, remova previamente o container relacionado antes de construir os demais, evitando recursos desnecess√°rios do docker-compose.yml antes de executar o docker compose up -d --build.

Caso seu ambiente possua suporte a CUDA, √© poss√≠vel executar o Ollama com acelera√ß√£o por GPU. Para isso, adicione o seguinte servi√ßo ao seu docker-compose.yml:

```bash
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: resume_analyzer_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_vol:/ollama
    networks:
      - resume_analyzer_network
    entrypoint: [ "/usr/bin/bash", "pull-llama3.sh" ]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```
Certifique-se de que o driver NVIDIA e o NVIDIA Container Toolkit estejam instalados no host para que a GPU seja reconhecida no container.

---
